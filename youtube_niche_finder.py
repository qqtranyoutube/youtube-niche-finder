
import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
from urllib.parse import quote

st.set_page_config(page_title="YouTube Niche Finder", layout="wide")

st.title("ğŸ” YouTube Niche Finder (MVP)")
st.markdown("Nháº­p chá»§ Ä‘á» gá»‘c vÃ  tÃ¬m cÃ¡c keyword Ä‘ang tÄƒng trÆ°á»Ÿng trÃªn YouTube.")

# BÆ°á»›c 1: Nháº­p tá»« khÃ³a gá»‘c
topic = st.text_input("ğŸ¯ Nháº­p chá»§ Ä‘á» (vÃ­ dá»¥: ai, fitness, crypto)", value="ai")

# BÆ°á»›c 2: Láº¥y gá»£i Ã½ tá»« khÃ³a tá»« YouTube Suggest (qua Google Suggest API)
@st.cache_data
def get_keyword_suggestions(query):
    url = f"http://suggestqueries.google.com/complete/search?client=firefox&ds=yt&q={quote(query)}"
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers)
    if r.status_code == 200:
        return r.json()[1]
    else:
        return []

# BÆ°á»›c 3: Láº¥y thÃ´ng tin video tá»« káº¿t quáº£ tÃ¬m kiáº¿m YouTube
def get_video_data(keyword, max_results=5):
    search_url = f"https://www.youtube.com/results?search_query={quote(keyword)}"
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(r.text, "html.parser")
    scripts = soup.find_all("script")
    for script in scripts:
        if 'var ytInitialData' in script.text:
            raw_json = script.text.strip().split(' = ', 1)[1].rsplit(";", 1)[0]
            break
    else:
        return []

    import json
    data = json.loads(raw_json)
    try:
        items = data['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents'][0]['itemSectionRenderer']['contents']
    except:
        return []

    results = []
    for item in items:
        video = item.get("videoRenderer")
        if not video: continue
        title = video['title']['runs'][0]['text']
        views_text = video.get("viewCountText", {}).get("simpleText", "0 views")
        published = video.get("publishedTimeText", {}).get("simpleText", "N/A")
        video_id = video["videoId"]
        link = f"https://www.youtube.com/watch?v={video_id}"
        results.append({
            "Keyword": keyword,
            "Title": title,
            "Views": views_text,
            "Published": published,
            "Link": link
        })
        if len(results) >= max_results:
            break
    return results

# Main logic
if topic:
    st.subheader("ğŸ“Œ Gá»£i Ã½ tá»« khÃ³a liÃªn quan")
    suggestions = get_keyword_suggestions(topic)
    if suggestions:
        st.write(", ".join(suggestions[:10]))
    else:
        st.error("KhÃ´ng láº¥y Ä‘Æ°á»£c tá»« khÃ³a gá»£i Ã½.")

    st.subheader("ğŸ“ˆ PhÃ¢n tÃ­ch video theo tá»« khÃ³a")
    all_results = []
    for kw in suggestions[:5]:
        videos = get_video_data(kw)
        all_results.extend(videos)

    if all_results:
        df = pd.DataFrame(all_results)
        st.dataframe(df, use_container_width=True)
        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("â¬‡ï¸ Táº£i káº¿t quáº£ CSV", data=csv, file_name="youtube_niche_results.csv")
    else:
        st.info("KhÃ´ng cÃ³ dá»¯ liá»‡u video phÃ¹ há»£p.")
